

1) Quick plain-English overview

Locally Weighted Regression fits a little linear model around each query point query_x, giving nearby training points more influence (higher weight) than far-away points. So instead of one global straight line, you get a smooth curve made by many local linear fits. The weights come from a Gaussian kernel (exp(-distance^2/(2*tau^2))), where tau is the bandwidth controlling how local the fit is.


---

2) Line-by-line walkthrough (what each syntax does)

import numpy as np
import matplotlib.pyplot as plt

numpy (aliased np) is the standard Python library for numerical arrays and linear algebra. It supports vectorized arithmetic and matrix operations.

matplotlib.pyplot (aliased plt) is the typical plotting module for creating charts.


def lwr(x, y, query_x, tau=0.5):

Defines a function lwr (Locally Weighted Regression).

Arguments:

x : 1-D numpy array of training input values (shape (n,)).

y : 1-D numpy array of target values (shape (n,)).

query_x : a single scalar input where we want a prediction.

tau : bandwidth (default 0.5) controlling weight fall-off.



X = np.c_[np.ones(len(x)), x]

np.ones(len(x)) creates a 1-D array of ones of length n.

np.c_[a, b] concatenates a and b column-wise. Result X becomes a 2-D array with shape (n, 2).

Each row of X is [1, x_i]. The 1 lets the model learn an intercept (bias) Œ∏‚ÇÄ. So our local model is ≈∑ = Œ∏‚ÇÄ + Œ∏‚ÇÅ * x.


W = np.diag(np.exp(-((x - query_x)**2) / (2 * tau**2)))

x - query_x subtracts the scalar query_x from each element of x (NumPy broadcasting) producing an array of distances.

(x - query_x)**2 squares elementwise.

np.exp(-(...)/(2 * tau**2)) applies the Gaussian kernel to each distance producing weights w_i = exp(-(x_i - query_x)^2 / (2 œÑ^2)).

np.diag(vector) produces an n √ó n diagonal matrix with those weights on the diagonal. This W is the weighting matrix for weighted least squares.


theta = np.linalg.pinv(X.T @ W @ X) @ X.T @ W @ y

@ is matrix multiplication in Python (NumPy). Important: A @ B performs linear-algebra multiplication.

X.T is the transpose of X. If X is (n, 2), then X.T is (2, n).

X.T @ W @ X yields a (2,2) matrix.

np.linalg.pinv(...) computes the Moore-Penrose pseudoinverse (safer than plain inverse; works if matrix is singular or near-singular).

The whole expression computes the weighted least-squares solution:


\theta = (X^\top W X)^{+} X^\top W y

theta is a length-2 vector: [Œ∏‚ÇÄ, Œ∏‚ÇÅ].


return np.array([1, query_x]) @ theta

np.array([1, query_x]) is the feature vector for the query point (shape (2,)).

Multiplying it by theta does Œ∏‚ÇÄ + Œ∏‚ÇÅ * query_x, a scalar prediction for this query.



---

3) The rest of the script (data generation & plotting)

np.random.seed(0)

Fixes the random-number generator seed so the noise is reproducible.


x = np.linspace(0, 10, 100)

np.linspace(start, stop, num) creates num evenly spaced values between start and stop. Here x is 100 points from 0 to 10 (shape (100,)).


y = np.sin(x) + 0.3 * np.random.randn(100)

np.random.randn(100) generates 100 samples drawn from the standard normal distribution (mean 0, variance 1).

Multiplying by 0.3 scales the noise (standard deviation 0.3).

y becomes a noisy sine wave (noisy targets).


xq = np.linspace(0, 10, 300)
yp = np.array([lwr(x, y, xi, tau=0.3) for xi in xq])

xq are 300 query points where we want predicted y to draw a smooth curve.

yp = np.array([...]) runs the lwr function for each xi in xq (list comprehension), collecting the scalar predictions into a 1-D array yp of length 300.

Note: this is repeated fitting ‚Äî for each xi we solve a weighted regression using the whole dataset. Computational cost scales with n * m (n training points and m query points).


Plotting:

plt.scatter(x, y, color='lightblue', label='Data')
plt.plot(xq, yp, color='red', label='LWR')
plt.title("Locally Weighted Regression")
plt.legend()
plt.grid()
plt.show()

plt.scatter draws the noisy training points.

plt.plot draws the predicted continuous curve through xq, yp.

plt.legend() shows the labels, plt.grid() adds grid lines, and plt.show() displays the plot.



---

4) Math & theory (derivation and intuition)

Weighted least squares problem

For a given query_x, we minimize a weighted squared error over Œ∏:

J(\theta) = \sum_{i=1}^{n} w_i \big(y_i - \theta^\top x_i\big)^2,

Set derivative to zero:

\frac{\partial J}{\partial \theta} = -2 X^\top W (y - X\theta) = 0

X^\top W X \,\theta = X^\top W y.

\theta = (X^\top W X)^{-1} X^\top W y.

\theta = (X^\top W X)^+ X^\top W y.

Why is it ‚Äúlocal‚Äù?

w_i is large when x_i is close to query_x, small otherwise.

So near query_x the local points dominate the fit, making the predicted value depend primarily on nearby data.


Kernel used

Gaussian kernel .

tau is the bandwidth (like œÉ in the Gaussian). Smaller tau ‚Üí narrower kernel ‚Üí more local weighting.



---

5) Practical intuition: bias, variance and the role of tau

Small tau: weights drop quickly with distance. Fit is highly local ‚Äî can follow noise closely (low bias, high variance). The curve will be very wiggly.

Large tau: weights fall slowly ‚Äî fit becomes closer to a global linear model (high bias, low variance). The curve will be smoother, perhaps too smooth.

Choose tau by experimentation or cross-validation.


6) Complexity and numerical notes

Current code builds an n √ó n matrix W for each query, but because your p = 2 (two parameters), the linear algebra mostly uses small 2√ó2 matrices. Still, building W is unnecessary and wastes memory/time.

Complexity: for m query points and n training points, cost is roughly O(m * n * p^2) ‚Äî here p=2 so effectively O(m * n).

Numerical stability: if X.T @ W @ X is nearly singular, pinv is good. Another option is regularization: add ŒªI to the matrix:


\theta = (X^\top W X + \lambda I)^{-1} X^\top W y

7) Useful improvements / alternatives / tips

Avoid building full W: compute the needed weighted sums directly. For p=2,


X^\top W X = \begin{bmatrix}
  \sum w_i & \sum w_i x_i \\
  \sum w_i x_i & \sum w_i x_i^2
  \end{bmatrix},
  \quad
  X^\top W y = \begin{bmatrix}
  \sum w_i y_i \\
  \sum w_i x_i y_i
  \end{bmatrix}

Nadaraya‚ÄìWatson (kernel regression): simpler alternative where prediction is the weighted average:


\hat y(\text{query}_x) = \frac{\sum_i w_i y_i}{\sum_i w_i}.

LOESS generalizes LWR to local polynomial fits (degree > 1) and commonly uses tricube weights and robustification steps.

Vectorization: list comprehension calls lwr for each xi and recomputes many things. For large m or n consider optimized implementations or compiled libraries.


8) Small worked shape-example (to make arrays concrete)

Suppose n = 3 training points x = [1, 2, 3]. Then:

X = [[1,1], [1,2], [1,3]] shape (3,2).

If query_x = 2, weights might be w = [exp(-1/2œÑ¬≤), 1, exp(-1/2œÑ¬≤)]. W is 3√ó3 diagonal.

Compute X.T @ W @ X ‚Üí a 2√ó2 matrix, invert/pinv it and multiply to get Œ∏ length-2.

Return 1 * Œ∏‚ÇÄ + 2 * Œ∏‚ÇÅ.

---

Got it üëç you‚Äôre stuck at the matrix representation part ‚Äî that‚Äôs very common!
Let‚Äôs carefully break it down with a tiny example so you see why matrices are used and how they just ‚Äúbundle‚Äù equations together.


---

1. Without matrices: write equations directly

Suppose we have 3 data points:

(x_1, y_1) = (1, 2),\quad (x_2, y_2) = (2, 3),\quad (x_3, y_3) = (3, 6).

We want a line:

\hat y = \theta_0 + \theta_1 x.

Weighted cost function:

J(\theta_0, \theta_1) = \sum_{i=1}^3 w_i \big(y_i - (\theta_0 + \theta_1 x_i)\big)^2.

This expands into:

J = w_1(y_1 - \theta_0 - \theta_1 x_1)^2 + w_2(y_2 - \theta_0 - \theta_1 x_2)^2 + w_3(y_3 - \theta_0 - \theta_1 x_3)^2

To minimize, we‚Äôd normally take derivatives wrt , set them to zero ‚Üí system of equations.
Messy, right? That‚Äôs where matrices help.


---

2. Bundle inputs into a matrix 

Each data point gives one equation of the form:

\hat y_i = \theta_0 \cdot 1 + \theta_1 \cdot x_i

Notice how there‚Äôs always a 1 (for intercept) and an .
So we can write:

\hat y_i = [1, x_i] \begin{bmatrix}\theta_0 \\ \theta_1\end{bmatrix}

For all 3 points together:

\begin{bmatrix}
\hat y_1 \\
\hat y_2 \\
\hat y_3
\end{bmatrix}
=
\begin{bmatrix}
1 & x_1 \\
1 & x_2 \\
1 & x_3
\end{bmatrix}
\begin{bmatrix}
\theta_0 \\
\theta_1
\end{bmatrix}

That big matrix is called design matrix .

For our numbers :

X =
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3
\end{bmatrix},\quad
\theta =
\begin{bmatrix}
\theta_0 \\
\theta_1
\end{bmatrix},\quad
y =
\begin{bmatrix}
2 \\
3 \\
6
\end{bmatrix}.

So compactly:

\hat y = X\theta


---

3. Adding weights

Weights are just a diagonal matrix:

W =
\begin{bmatrix}
w_1 & 0 & 0 \\
0 & w_2 & 0 \\
0 & 0 & w_3
\end{bmatrix}

This means when we compute errors, each is scaled differently.


---

4. Normal equation in plain form

From calculus (derivative wrt ) we get the solution:

\theta = (X^\top W X)^{-1} X^\top W y

What this is doing:

 = weighted correlation between inputs.

 = weighted correlation between inputs and outputs.

Inverse solves for parameters.



---

5. Why matrices?

Without matrices, you‚Äôd write 2 separate equations for  and .

With matrices, one compact formula works for any number of data points (3, 100, 1000, ‚Ä¶).

Also, computers can calculate matrix operations very efficiently.



---

‚úÖ So matrix notation is just a shorthand for solving many equations at once.


---

üëâ Do you want me to actually plug in numbers (choose weights, compute , invert it, etc.) so you can see how the formula spits out  step by step?


📘 Naive Bayes Spam Classifier — Doubts & Answers


---

1️⃣ What are X_train, X_test, y_train, y_test?

They come from train_test_split.


X_train, X_test, y_train, y_test = train_test_split(
    df['text'], df['label'], test_size=0.3, random_state=42
)

X_train → Training messages (input text).

y_train → Training labels (0 = ham, 1 = spam).

X_test → Testing messages (input text, unseen by model).

y_test → Testing labels (answers for test data).


✅ Example after splitting:

X_train: “Hey, are we still on for dinner tonight?”, “Lunch tomorrow?”, …

y_train: [0, 0, …]

X_test: “WINNER! You have won a free cruise. Call now!”, …

y_test: [1, …]



---

2️⃣ What does this mean?

vectorizer = CountVectorizer(stop_words='english')
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

CountVectorizer → Converts text into numbers (bag of words model).

stop_words='english' → Removes common useless words like "is", "the".

fit_transform(X_train) →

fit = learns all unique words (vocabulary).

transform = converts training text into word count vectors.


transform(X_test) → Converts test text using same vocabulary (no new learning).


✅ Example:
Vocabulary → [account, call, free, win]
Message “Win free call” → [0,1,1,1]


---

3️⃣ Why do we use fit again here?

model = MultinomialNB()
model.fit(X_train_vec, y_train)

This time, the object is Naive Bayes classifier, not the vectorizer.

fit means learn:

Learns word probabilities for spam vs ham.

Example: word “win” appears 90% in spam → strong spam indicator.



👉 Two different learners:

1. vectorizer.fit → learns vocabulary (words → numbers).


2. model.fit → learns classification (numbers → spam/ham).




---

4️⃣ What is :.2f in f-strings?

print(f"Accuracy: {accuracy:.2f}")

:.2f means format as float with 2 decimal places.

Example:

3.14159 → 3.14

0.85714 → 0.86



✅ Used to make results cleaner (instead of long decimals).


---

5️⃣ What does this syntax mean?

for msg, pred in zip(test_messages, predictions):
    print(f"'{msg}' => {'spam' if pred==1 else 'ham'}")

zip(test_messages, predictions) → pairs each message with its prediction.

f-string inserts message and prediction into a string.

'spam' if pred==1 else 'ham' → short inline if-else:

If prediction = 1 → spam.

If prediction = 0 → ham.



✅ Example Output:

'Your loan is approved! Call now to claim.' => spam
'Hi, just checking in to see how you're doing.' => ham


---

🎯 Quick Flow of the Whole Code

1. Dataset → messages + labels (ham/spam).


2. Split → training set (to learn), test set (to evaluate).


3. Vectorize → convert text → numbers.


4. Train → Naive Bayes model learns spam vs ham.


5. Evaluate → print accuracy, precision, recall.


6. Predict new messages → print whether spam/ham.




---

✅ This covers all your doubts step by step.
Would you like me to also make a one-page cheatsheet/diagram that visually shows the pipeline (Text → Vectorizer → Model → Prediction)?

